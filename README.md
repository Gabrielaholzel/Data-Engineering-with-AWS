## ðŸŽ“ Data Engineering Nanodegree Projects

Hello! I'm **Gabriela Holzel**, a Ssr Data Engineer and newly graduated Data Engineer from [Udacity's](https://www.udacity.com/) [Data Engineering Nanodegree](https://www.udacity.com/course/data-engineer-nanodegree--nd027). This program has been an incredible journey, allowing me to polish the skills and knowledge needed to excel in the field of data engineering. I've worked on four comprehensive projects that showcase my ability to create scalable, efficient data solutions using modern technologies. Here's an overview of the projects and what I've learned along the way.

## ðŸŒŸ Projects Overview

### 1. Data Modeling with Cassandra

#### Project Summary
In this project, I worked with a startup called Sparkify to help them understand their song play data. Sparkify needed a way to efficiently query their data, which resided in a directory of CSV files. I built a Cassandra database to model their data, allowing them to perform complex queries with ease.

#### Key Skills:
- **NoSQL Data Modeling**: Designed and implemented a Cassandra database schema tailored to specific analytical queries.
- **ETL Pipeline**: Developed an ETL pipeline to transform and load data into the Cassandra database.
- **Apache Cassandra**: Leveraged the power of Cassandra for distributed data storage and retrieval.

#### Notable Achievements:
- Created tables optimized for query patterns.
- Enabled efficient retrieval of song play data.

### 2. Cloud Data Warehouses

#### Project Summary
For this project, I built an ETL pipeline to move Sparkify's growing dataset from S3 to Redshift. This involved staging the data in Redshift and transforming it into a set of dimensional tables for analytics.

#### Key Skills:
- **Cloud Data Warehousing**: Utilized Amazon Redshift to create a scalable data warehouse.
- **ETL Pipeline**: Built a robust ETL pipeline to extract data from S3, stage it in Redshift, and transform it.
- **AWS Services**: Gained proficiency in using AWS services like S3 and Redshift.

#### Notable Achievements:
- Successfully designed and implemented dimensional tables for optimized queries.
- Ensured data integrity and performance with efficient data transformations.

### 3. Data Lake with Spark

#### Project Summary
In this project, I worked on building a data lake for the STEDI Human Balance Analytics team. The goal was to create a lakehouse architecture to manage sensor data, enabling the training of machine learning models.

#### Key Skills:
- **Big Data Ecosystem**: Learned about the components and architecture of a data lakehouse.
- **Apache Spark**: Utilized Spark for data wrangling and transformations.
- **AWS Glue and S3**: Integrated AWS services to manage and process data efficiently.

#### Notable Achievements:
- Built an ELT pipeline to process and transform data using Spark.
- Created a data lakehouse that supports complex data queries and machine learning workflows.

### 4. Automating Data Pipelines with Airflow

#### Project Summary
For the final project, I designed high-grade data pipelines using Apache Airflow. These pipelines moved JSON logs of user activity and metadata from S3 to Redshift, ensuring data quality and monitoring throughout the process.

#### Key Skills:
- **Data Pipelines**: Developed reusable and automated data pipelines using Airflow.
- **Airflow DAGs**: Created Directed Acyclic Graphs (DAGs) to manage task dependencies and execution.
- **Data Quality**: Implemented checks to ensure data integrity and consistency.

#### Notable Achievements:
- Automated data ingestion and processing with custom Airflow operators.
- Enhanced pipeline reliability with robust data quality checks.

## ðŸ’¡ What I've Practised 

Throughout the nanodegree, I've reinforced my expertise in:

- **Relational and NoSQL Data Models**: Creating user-friendly and efficient data models.
- **Data Warehousing**: Building scalable data warehouses in the cloud.
- **Big Data Processing**: Working with massive datasets using tools like Spark.
- **Data Lakes**: Designing and interacting with cloud-based data lakes.
- **Data Pipeline Automation**: Automating and monitoring data workflows with Airflow.

## ðŸš€ Let's Connect

I'm excited to bring these skills to new challenges and opportunities. Feel free to check out my projects in detail on my [GitHub](https://github.com/Gabrielaholzel). Let's connect on [LinkedIn](https://www.linkedin.com/in/gabrielaholzel/) or reach out via email at gabrielaholzel@gmail.com. 

Thank you for visiting my portfolio, and I look forward to connecting with you! ðŸš€

---

Gabriela Holzel
Data Engineer | Udacity Nanodegree Graduate
